# -*- coding: utf-8 -*-
"""Unsupervised_Learning_Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rDLRBxSOvnx6k8LJuYC2kySd7NCoRj08

#Task 1
"""

import pandas as pd
import numpy as np

df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00488/Live_20210128.csv")
df.head()

print(df.info)
print("Observation 1:","Column 1, column 2, column 3 and column 4 are empty")
print("Observation 2:","This dataframe consists of 7050 rows and 16 columns")

df.isnull().sum()

df["status_type"].unique()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["status_type"] = le.fit_transform(df["status_type"])
df["status_published"] = le.fit_transform(df["status_published"])
df

"""#Task 2

"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
d = ["num_reactions","num_comments","num_shares","num_likes","num_loves","num_wows","num_hahas","num_sads","num_angrys"]
df[d]= scaler.fit_transform(df[d])
df

from sklearn.cluster import KMeans
from sklearn import metrics
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt

x1 = np.array(df["num_reactions"])
x2 = np.array(df["num_comments"])


plt.plot()
X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)

distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k).fit(X)
    kmeanModel.fit(X)
    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])

plt.plot(K, distortions, "bx-")
plt.xlabel("k")
plt.title("Elbow Method")
plt.show()

from sklearn.metrics import silhouette_score

sil = []
kmax = 10

for k in range(2, kmax+1):
  kmeans = KMeans(n_clusters = k).fit(df[d])
  labels = kmeans.labels_
  sil.append(silhouette_score(df[d], labels, metric = 'euclidean'))
plt.plot(sil)
plt.show()

from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
cluster.fit_predict(df[d])

from sklearn.metrics.cluster import adjusted_rand_score
adjusted_rand_score([0, 0, 1, 1], [1, 1, 1, 1])

"""#Task 3"""

from sklearn.decomposition import PCA
df1 = pd.read_csv("https://raw.githubusercontent.com/dphi-official/Datasets/master/fruit_data.csv")
pca = PCA(n_components=2)
pca.fit(df1)
pca.components_

from sklearn.preprocessing import normalize

df1_scaled = normalize(df1)
df1_scaled = pd.DataFrame(df1_scaled, columns=df1.columns)
df1_scaled

from sklearn.decomposition import PCA

pca = PCA(n_components=4)
pca.fit(df1_scaled)
print(pca.components_)
print('\nVariance of each component:', pca.explained_variance_ratio_)